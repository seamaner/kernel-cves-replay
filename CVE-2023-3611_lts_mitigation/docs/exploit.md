Exploit Details
===============

Exploit demo for CVE-2023-3611.
Flags:
- `kernelCTF{v1:lts-6.1.35:1688135277:098358ab92b525678de0c42817048d693400c68e}`
- `kernelCTF{v1:mitigation-6.1-v2:1688982924:6bdfebbb5220c89a748c8c7a0fbeec5d34988932}`

Note: The original mitigation exploit targeted the `mitigation-6.1-broken` instance, it was later modified to work on `mitigation-6.1-v2`.


# LTS
## Summary

At a high level the exploit performs the following:

- Trigger the vulnerability into a user controlled buffer, leaving us with a controllable `struct qfq_group` object
- Choose the object in such a way that we can trigger an OOB write into `struct qfq_sched`
- Use this write to achieve an UaF in the `kmalloc-128` cache
- Pivot to `struct tcf_proto` objects to obtain an KASLR leak as well as RIP control when we reclaim the object
- Pivot the stack back into a user controlled buffer in the `kmalloc-8192` cache.

## Steps in Detail

### Step 1: Abusing the Vulnerability

Triggering the vulnerability is trivial, though actually taking something useful
out of the out-of-bounds group is not obvious.
Even though the pointer is used in a few places I only identified few places which
seem particularly interesting from a exploitation perspective. We will focus on
the following:
```c
static void qfq_schedule_agg(struct qfq_sched *q, struct qfq_aggregate *agg)
{
	struct qfq_group *grp = agg->grp;
// ...
	s = qfq_calc_state(q, grp);
	__set_bit(grp->index, &q->bitmaps[s]); // [1]
// ...
}
```

Looking at `qfq_schedule_agg` we can see that we have a bit set operation at a controlled index [1].
The idea will be to flip a bit on a pointer to eventually cause a type confusion.
Looking at the `struct qfq_sched` we can see that there are only few members
available to us after the `bitmaps[]` member (we cannot use a negative index).

```c
struct qfq_sched {
	struct tcf_proto __rcu *filter_list;
	struct tcf_block	*block;
	struct Qdisc_class_hash clhash;

	u64			oldV, V;	/* Precise virtual times. */
	struct qfq_aggregate	*in_serv_agg;   /* Aggregate being served. */
	u32			wsum;		/* weight sum */
	u32			iwsum;		/* inverse weight sum */

	unsigned long bitmaps[QFQ_MAX_STATE];	    /* Group bitmaps. */
	struct qfq_group groups[QFQ_MAX_INDEX + 1]; /* The groups. */
	u32 min_slot_shift;	/* Index of the group-0 bit in the bitmaps. */

	u32 max_agg_classes;		/* Max number of classes per aggr. */
	struct hlist_head nonfull_aggs; /* Aggs with room for more classes. */
};
```

The `nonfull_aggs` member is interesting as this list will be used to lookup
aggregates when creating new classes:
```c
static struct qfq_aggregate *qfq_find_agg(struct qfq_sched *q,
					  u32 lmax, u32 weight)
{
	struct qfq_aggregate *agg;

	hlist_for_each_entry(agg, &q->nonfull_aggs, nonfull_next)
		if (agg->lmax == lmax && agg->class_weight == weight)
			return agg;

	return NULL;
}
```
The idea will be to smuggle a fake `qfq_aggregate` into the qdisc which is
hopefully freed when destroying the class that possesses it.
This way we can have a UaF in the `kmalloc-128` slab.
A suitable fake `qfq_aggregate` needs to allow full control of the relevant members
(`lmax`, `class_weight`, `num_classes`).
In order to smuggle the fake object, we will flip a bit of the `nonfull_aggs` list
member to hopefully point to our controlled object:
```

+-qfq qdisc----+
| ...          |
| filter_list  |
| ...          |                                   +-fake object---+
| bitmaps[]    |                                   |               |
| groups[]     |                                   |               |
| ...          |                                   +---------------+
| nonfull_aggs | -------x-------------+
+--------------+        x             |
                        x             +-----> 0200 +-qfq_aggregate-+
+-controlled o-+        x flip                     |               |
| ...          |        x a bit                    |               |
| fake grp {}  |        x                          +---------------+
|              |        x
| ...          |        x-------------------> 0280 +-fake object---+
| ...          |                                   |               |
|              |                                   |               |
+--------------+                                   +---------------+
```

By targeting a bit of the `nonfull_aggs` member of the qdisc object we will
potentially be able to inject a fake `struct qfq_aggregate` object into the
qdisc.

#### Step 1.1: QFQ Internal State Control

Looking at the code in `qfq_change_class()` we can see that `qfq_add_to_agg()`
is called with the new `agg` after triggering the vulnerability:

```c
// net/sched/sch_qfq.c

/* Add class to aggregate. */
static void qfq_add_to_agg(struct qfq_sched *q,
			   struct qfq_aggregate *agg,
			   struct qfq_class *cl)
{
	cl->agg = agg;

	qfq_update_agg(q, agg, agg->num_classes+1); // [1]
	if (cl->qdisc->q.qlen > 0) { /* adding an active class */
		list_add_tail(&cl->alist, &agg->active);
		if (list_first_entry(&agg->active, struct qfq_class, alist) ==
		    cl && q->in_serv_agg != agg) /* agg was inactive */
			qfq_activate_agg(q, agg, enqueue); /* schedule agg */   // [2]
	}
}
```

After the out-of-bound group is stored into the aggregate in [1] we can
hit `qfq_activate_agg()` [2].

```c
/* Update agg ts and schedule agg for service */
static void qfq_activate_agg(struct qfq_sched *q, struct qfq_aggregate *agg,
			     enum update_reason reason)
{
	agg->initial_budget = agg->budget = agg->budgetmax; /* recharge budg. */

	qfq_update_agg_ts(q, agg, reason);
	if (q->in_serv_agg == NULL) { /* no aggr. in service or scheduled */
		q->in_serv_agg = agg; /* start serving this aggregate */
		 /* update V: to be in service, agg must be eligible */
		q->oldV = q->V = agg->S;
	} else if (agg != q->in_serv_agg)
		qfq_schedule_agg(q, agg); // [3]
}
```

After passing the checks in `qfq_activate_agg()` we will call the desired
`qfq_schedule_agg()` [3].

In order to hit these code paths we need to fullfill certain constraints:
1. `q->in_serv_agg != NULL` and `q->in_serv_agg != new_oob_agg`
2. (sub) qdisc of the owning class of the aggregate needs to be non-empty (`cl->qdisc->q.qlen > 0`)

We can control `q->in_serv_agg` by enqueuing packets:
```c
static int qfq_enqueue(struct sk_buff *skb, struct Qdisc *sch,
		       struct sk_buff **to_free)
{
// ...
	qfq_activate_agg(q, agg, enqueue);
// ...
}
```
Initially `q->in_serv_agg` will be `NULL`, thus we will hit the second branch
in `qfq_activate_agg()` (see above).

The problem is that, right after enqueuing the packet, the dequeue operation
will reset the state (unless we generate enormous amounts of traffic so that
the scheduling actually kicks in, which however still leaves us with a race).
In order to work around that problem we will modify the sub qdisc of the class
to be a `netem` qdisc, which allows us to add a generously chosen delay, so that
the dequeue operation fails because no packet is available yet.
This will issue a warning in `qfq_peek_skb()`, but that will not be problem for us.

This solves constraint number one. As a bonus this naturally solves constraint
number two because the underlaying netem qdisc has in fact packets queued,
they are just delayed.

We need to send packets to the qdisc to trigger the vulnerability anyway, so
we can combine those goals.
After corrupting an aggregate using the vulnerability, the qdisc is in a very unstable
state, so we have to make sure, that the packet that triggers the vulnerability is dropped
(and the ones after, too).
This will prevent further calls to `qfq_activate_agg()` and a lot of code paths in
`qfq_dequeue()` which are likely to make the kernel panic with the fake group in place.
In order to achieve that we will set the limit for the configured `netem` qdisc
accordingly, dropping packets after the limit was reached.

Finally, also note that we setup the sizetable in such a way that it performs a table lookup
to get the resulting packet size. This way we can choose our packets accordingly
in order to trigger the vulnerability only when we want to.

### Step 2: Heap Spray

#### Step 2.1: QFQ Qdiscs and `kmalloc-8192`

To successfully make use of the vulnerability we need a controllable object in
the `kmalloc-8192` cache.

The qdisc is allocated by `qdisc_alloc()`:
```c
// qdisc_alloc() in net/sched/sch_generic.c
	struct Qdisc *sch;

// ..

	dev = dev_queue->dev;
	sch = kzalloc_node(size, GFP_KERNEL, netdev_queue_numa_node_read(dev_queue));
```

Thus we need an object which is allocated using `GFP_KERNEL` as well.
Though the choice might be questionable in hindsight, I chose the well known
`struct user_key_payload` for this purpose, as this structure can be allocated
with variable sizes in 24 < 32767 + 24.
One downside of the key structure is the fact that we can only allocate a few
of them because our quota is (by default) limited to 20000 bytes.
Besides that we can easily control a `qfq_group` object at our desired offset
according to the `lmax` we set when triggering the vulnerability:
```c
struct qfq_group {
	u64 S, F;			/* group timestamps (approx). */
	unsigned int slot_shift;	/* Slot shift. */
	unsigned int index;		/* Group index. */
	unsigned int front;		/* Index of the front slot. */
	unsigned long full_slots;	/* non-empty slots */

	/* Array of RR lists of active aggregates. */
	struct hlist_head slots[QFQ_MAX_SLOTS];
};
```

Because of the restrictions we can only spray 3-4 fake objects which greatly
reduces the chances of hitting the correct object when triggering the vulnerability.

We will spray several qdiscs to groom the heap.
After that we will spray a few key payloads and hope that one of them lands right after
the last qdisc we sprayed.
The enqueue operation is naturally prone to kernel panics, and unluckily especially
when the fake object is containing zeros only, which is somewhat likely in the
rather less used `kmalloc-8192`.

There are several strategies to make it more stable, but we are happy with the
roughly 25% of the naive approach.

In order to spray qdiscs we will create child processes, each with a new
network namespace to easily generate loopback devices to attach qdiscs to.
The only real requirement of the payload is to have the correct `grp->index`
set. Additionally keeping all other fields zero ensures that adding nodes to the
`grp->slots[]` lists does not cause any troubles.

An advantage of using `struct user_key_payload` is the fact, that we know when the heap
spray succeeded because `qfq_slot_insert()` will add the aggregate pointer into the
`slots` array of the group we control:

```c
static void qfq_slot_insert(struct qfq_group *grp, struct qfq_aggregate *agg,
			    u64 roundedS)
{

// ..

	hlist_add_head(&agg->next, &grp->slots[i]);
	__set_bit(slot, &grp->full_slots);
}
```

With the key control we can then read the key back and check if we got a kernel
heap pointer.
As a bonus, we can check if the pointer has the relevant bit that we want to flip
unset, so that we know whether the bitflip succeeded.
If it failed and we got that far, we can easily retry.


#### Step 2.2: qfq_aggregate and `kmalloc-128`

Since we already utilized `struct user_key_payload`, we will do so again.
The payloads are much smaller now, too, so we do not have to worry about quota restrictions.

Looking at the `struct qfq_aggregate` we can see that all the relevant fields can
be controlled with a small key payload:
```c
struct qfq_aggregate {
  struct hlist_node          next;                 /*     0    16 */
  u64                        S;                    /*    16     8 */

  // start of user controllable data:
  u64                        F;                    /*    24     8 */
  struct qfq_group *         grp;                  /*    32     8 */
  u32                        class_weight;         /*    40     4 */
  int                        lmax;                 /*    44     4 */
  u32                        inv_w;                /*    48     4 */
  u32                        budgetmax;            /*    52     4 */
  u32                        initial_budget;       /*    56     4 */
  u32                        budget;               /*    60     4 */
  /* --- cacheline 1 boundary (64 bytes) --- */
  int                        num_classes;          /*    64     4 */

  u8 __pad0[4]; /* XXX 4 bytes hole, try to pack */

  struct list_head           active;               /*    72    16 */
  struct hlist_node          nonfull_next;         /*    88    16 */

  /* size: 104, cachelines: 2, members: 13 */
  /* sum members: 100, holes: 1, sum holes: 4 */
  /* last cacheline: 40 bytes */
}
```
By choosing a unique `lmax` for this aggregate we can identify it later.
Keeping the list heads `NULL` will be fine, since they will be initialized
by the kernel code if they are not yet.
Leaving `grp` equal to `NULL` will make the kernel code overwrite it with a
pointer into the `q` as well (see `qfq_update_agg()`).
We will choose `num_classes` to be equal to 0 so that deletion of the
to-be-owning class will cause the fake aggregate to be freed.

This will achieve a couple of things:
- We can craft a fake aggregate without needing any knowledge of kernel pointers
  (even though we would know the pointer of the aggregate we are faking).
- We have a unique identifier to make sure that the spray was successful before
  getting the kernel unstable
- When reading back the corrupted key payload, we have another leak to kernel heap
  memory. Specifically the `kmalloc-8192` cache this time. This allows us to
  precisely locate our initial key buffer, too.

We will spray a few fake aggregates before triggering the vulnerability
and a few after the trigger. This way we hopefully increase our chances to
win the bit flip.


### Step 3: Trigger Use-after-Free

To recap, at this point we have the following:
- A QFQ qdisc in `kmalloc-8192`, right behind it a user controlled buffer
    - we know the pointers of both of them
- A `qfq_class` with a fake `qfq_aggregate` which is actually a `user_key_payload` we control
    - we know the pointer of this buffer as well

We can now free the aggregate by deleting the class we attached it to.

Looking for objects which can be used to reclaim the freed aggregate I found
the `struct tcf_proto` (in `include/net/sch_generic.h`):
```c
struct tcf_proto {
  void*         next;                 /*     0     8 */
  void *                     root;                 /*     8     8 */
  int                        (*classify)(struct sk_buff *, const struct tcf_proto  *, struct tcf_result *); /*    16     8 */

  // start of user controllable data
  u16                     protocol;             /*    24     2 */

  /* XXX 2 bytes hole, try to pack */
  u8 __pad0[2];

  u32                        prio;                 /*    28     4 */
  void *                     data;                 /*    32     8 */
  const struct tcf_proto_ops  * ops;               /*    40     8 */
  struct tcf_chain *         chain;                /*    48     8 */
  u32                 lock;                 /*    56     4 */
  u8                       deleting;             /*    60     1 */

  /* XXX 3 bytes hole, try to pack */
  u8 __pad1[3];

  /* --- cacheline 1 boundary (64 bytes) --- */
  u32                 refcnt;               /*    64     4 */

  /* XXX 4 bytes hole, try to pack */
  u8 __pad2[4];

  u8       rcu[16];
  struct hlist_node          destroy_ht_node;      /*    88    16 */

  /* size: 104, cachelines: 2, members: 13 */
  /* sum members: 95, holes: 3, sum holes: 9 */
  /* forced alignments: 1, forced holes: 1, sum forced holes: 4 */
  /* last cacheline: 40 bytes */
};
```

By reclaiming the freed `struct qfq_aggregate` with this object, we will be able
to leak a pointer which allows us to bypass KASLR (`*ops`).
Additionally we can later overwrite the `ops` to gain RIP control.
This structure is particularly well suited for this, as you will see later.

Leaking the structure contents turns out to be non-trivial.
The `classify` member of the `struct tcf_proto` (a shortcut to `ops->classify`)
will overlap with the size field of `struct user_key_payload` at offset 16.
This size field is a `u16`, thus the lowest to bytes of the `classify` function pointer.
The problem is that `keyctl_read_key` in `security/keys/keyctl.c` will either read
the whole key or no key at all.
This means have to survive a kernel heap read up to the size specified by the kernel
pointer.
Especially with the guard pages present, this will likely fail.
In order to circumvent this problem we will choose a classifier with a "low" address.
Specifically we choose `rsvp` because it has the lowest available:
```
$ cat kallsyms | grep -Ew "basic_classify|cls_bpf_classify|cls_cgroup_classify|fw_classify|route4_classify|u32_classify|rsvp_classify"
ffffffff89cdba10 t u32_classify
ffffffff89cdef10 t route4_classify
ffffffff89cdf300 t fw_classify
ffffffff89ce1710 t rsvp_classify
ffffffff89ce27d0 t basic_classify
ffffffff89ce3320 t cls_cgroup_classify
ffffffff89ce3c30 t cls_bpf_classify
```

This means we only have to copy `0x1710` bytes which works well practically.

### Step 4: Getting RIP Control

By overwriting the `ops` member of the aforementioned `struct tcf_proto` we are
on a good way to gain arbitrary kernel code execution.

Looking through references to the available function pointers I chose the path
through `tc_get_tfilter()`:

```c
static int tc_get_tfilter(struct sk_buff *skb, struct nlmsghdr *n,
			  struct netlink_ext_ack *extack) {
	struct tcf_proto *tp = NULL;

// ...

	tp = tcf_chain_tp_find(chain, &chain_info, protocol,
			       prio, false); // [1]

// ...

	fh = tp->ops->get(tp, t->tcm_handle); // [2]

	if (!fh) {
		NL_SET_ERR_MSG(extack, "Specified filter handle not found");
		err = -ENOENT;
	} else {
		err = tfilter_notify(net, skb, n, tp, block, q, parent,
				     fh, RTM_NEWTFILTER, true, rtnl_held); // [3]
		if (err < 0)
			NL_SET_ERR_MSG(extack, "Failed to send filter notify message");
	}

// ...

}

static int tcf_fill_node(struct net *net, struct sk_buff *skb,
			 struct tcf_proto *tp, struct tcf_block *block,
			 struct Qdisc *q, u32 parent, void *fh,
			 u32 portid, u32 seq, u16 flags, int event,
			 bool terse_dump, bool rtnl_held)
{

// ...

		// [4]
		if (tp->ops->dump &&
		    tp->ops->dump(net, tp, fh, skb, tcm, rtnl_held) < 0)
			goto nla_put_failure;

// ...
}
```

First the structure is retrieved [1]. We will have a closer look at this later.
After that the `ops->get()` function is invoked, with `rdi` pointing to the buffer
itself and `rsi` pointing to a handle we specified.

I could not find a proper stack pivoting gadget which would jump to memory controlled
by `rdi` so we will pursue a different idea:
We will set `ops->get` to point to a gadget which will simply return `rdi`.
This way, we will eventually call `tcf_fill_node()` in `tfilter_notify()` [3].
In `tcf_fill_node()` `ops->dump()` [4] is invoked, now with more control over
the parameters. Even though we have useful pointers available in both `rsi`
and `rdx`, the following stack pivot will only use `rsi`:

We will set `ops->dump()` to the following gadget:
```nasm
push rsi;
jmp [rsi + 0x39]
```

With this gadget we can perform a stack pivot into our fake `struct tcf_proto`.
Since this object is quite small (and some fields are constrained), we will
pivot the stack again.
Naturally a good target is the large buffer in `kmalloc-8192` because we already
know the address and we have plenty of space there.
Note that we cannot modify the key payload, thus have to free it and repeat the
spray.
Also, note that the `ops` member is introducing one level of pointer indirection,
but we will use the large buffer for that, too.

In order to do the first pivot, we choose the following gadget chain:
```nasm
pop rsp;
add rsp, 0x18;
pop rbx;
pop rbp;
pop r12;
ret;
```
This chain is specifically chosen to overlay the `struct tcf_proto` in such a way
that it does not touch any vital members.

We can control `rbp` with this chain, thus we use the following gadget to pivot
to the prepared stack in the large buffer:
```nasm
mov rsp, rbp;
pop rbp;
ret
```

After that we are on a large stack without any constraints, thus assembling a
standard ROP payload escalating privileges and escaping the sandbox is trivial.

#### Step 4.1: Stabilizing the Trigger

As mentioned earlier `struct tcf_proto` is a very good choice as an
overwrite target.
Looking at the way the fake structure is retrieved we will see that we
can make use of the error conditions in order to make the trigger completely stable.

```c
// in tc_get_tfilter()
	if (!tp || IS_ERR(tp)) {
		NL_SET_ERR_MSG(extack, "Filter with specified priority/protocol not found");
		err = tp ? PTR_ERR(tp) : -ENOENT; // [1]
		goto errout;
	} else if (tca[TCA_KIND] && nla_strcmp(tca[TCA_KIND], tp->ops->kind)) {
		NL_SET_ERR_MSG(extack, "Specified filter kind does not match existing one");
		err = -EINVAL; // [2]
		goto errout;
	}
```

By choosing a unique protocol and priority for the fake `tcf_proto` we can
prevent wrong structures from being found.
If the kernel cannot find the requested protocol / priority it will issue an
`ENOENT` [1]. By catching this error we know whether the initial spray
of the fake `tcf_proto` succeeded.
If it did not, we can simply retry.

When the kernel found the fake `tcf_proto`, it will compare `tp->ops->kind` to
the requested kind. By choosing a unique kind here as well, we will be able
to tell whether the spray of the large objects succeeded because we can
observe the error `EINVAL` if it was wrong.
No code will be executed before that.


## General Notes on the Exploit

The exploit makes heavy use of multiprocessing in order to simplify the use
of the network namespaces (we use one network namespace for each QFQ qdisc we
create.)

The main function of the exploit performs coordination of the child processes.
The children will notify the parent through a simple wait based event system.
For the first 3 steps a process is repeatedly cloned into `bug_worker()` until
we identified one worker which is actually triggering the vulnerability (still
inside that worker).
Finally, the spraying of `struct tcf_proto` and eventually triggering the RIP
control is done by another worker process `final_stage_worker()`.

There is one additional worker process which is handling everything related to
the sprayed keys. We need this process because keys are generally scoped by
permissions. To not care about that we have one dedicated process which owns
all the keys and just acts on demand.

Finally note that the exploit does not make use of any netlink library or the like.
Therefor, you may notice that the code related to netlink is quite verbose.

### Stability

The main problem is the initial heap spray where we try to land a large key payload
after the victim qdisc.
Because we only have 3 key payloads available we are unlikely to hit this scenario.
Depending on the instance the success rate was about 30-50%.
I later discovered that this step could be made much more stable by using different
CPUs for the main orchestration and the workers.
Additionally using another object to perform the `kmalloc-8192` spray may be
beneficial, because the limited quota greatly decreases the chances of hitting
a "good" layout. One such object would be `struct qdisc_size_table` as briefly
described in the related mitigation exploits.

The last steps are quite stable and work almost all of the time.

Finally, one should note that the exploit is not performing a decent
post-exploitation cleanup.
The vulnerable QFQ qdisc class is not properly cleaned up, thus
as soon as the timers for dequeue operations fire, the kernel will likely
panic.


# Mitigation

The exploit for the mitigation instance largely follows that of CVE-2023-31436,
thus I will not go into too much detail on it.
The main difference is the way we trigger the vulnerability.
(The exploit for CVE-2023-31436 was obviously designed to work for both).

In summary, instead of the process described above we will directly corrupt the
`filter_list` member of `qfq_sched`.
We need a three-way layout in order for that to work (we are targeting the qdisc
which lies after the controlled buffer).
This will cause a type confusion for the `struct tcf_proto` object which we trivially
expand into RIP control using `struct xdp_umem`.

Triggering the vulnerability imposes additional constraints on the sprayed sizetables.
Since the group object we want to control is far into the sizetable this is no
problem and can be setup similarly to the LTS version.
